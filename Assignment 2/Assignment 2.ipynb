{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import sklearn\n",
    "from hypopt import GridSearch\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_unigram = CountVectorizer(analyzer='word', ngram_range = (1, 1))\n",
    "vect_bigram = CountVectorizer(analyzer='word', ngram_range = (2, 2))\n",
    "vect_unibigram = CountVectorizer(analyzer='word', ngram_range = (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    pos_train = pd.read_csv(os.path.join(file_path + \"\\\\pos_train.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    neg_train = pd.read_csv(os.path.join(file_path + \"\\\\neg_train.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    pos_test = pd.read_csv(os.path.join(file_path + \"\\\\pos_test.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    neg_test = pd.read_csv(os.path.join(file_path + \"\\\\neg_test.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    pos_vali = pd.read_csv(os.path.join(file_path + \"\\\\pos_vali.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    neg_vali = pd.read_csv(os.path.join(file_path + \"\\\\neg_vali.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    pos_train_ns = pd.read_csv(os.path.join(file_path + \"\\\\pos_train_ns.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    neg_train_ns = pd.read_csv(os.path.join(file_path + \"\\\\neg_train_ns.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    pos_test_ns = pd.read_csv(os.path.join(file_path + \"\\\\pos_test_ns.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    neg_test_ns = pd.read_csv(os.path.join(file_path + \"\\\\neg_test_ns.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    pos_vali_ns = pd.read_csv(os.path.join(file_path + \"\\\\pos_vali_ns.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    neg_vali_ns = pd.read_csv(os.path.join(file_path + \"\\\\neg_vali_ns.csv\"), sep = 'delimiter', names = ['Review'], engine = 'python')\n",
    "    return pos_train,neg_train,pos_test,neg_test,pos_vali,neg_vali,pos_train_ns,neg_train_ns,pos_test_ns,neg_test_ns,pos_vali_ns,neg_vali_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(x, y, count_vect, a):\n",
    "    x_count = count_vect.fit_transform(x)\n",
    "    tfidf = TfidfTransformer()\n",
    "    x_tfidf = tfidf.fit_transform(x_count)\n",
    "    clf = MultinomialNB(alpha = a).fit(x_tfidf, y)\n",
    "    return clf, count_vect, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x, y, clf, count_vect, tfidf):\n",
    "    x_count = count_vect.transform(x)\n",
    "    x_tfidf = tfidf.transform(x_count)\n",
    "    preds = clf.predict(x_tfidf)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y, preds),\n",
    "        'precision': precision_score(y, preds),\n",
    "        'recall': recall_score(y, preds),\n",
    "        'f1': f1_score(y, preds),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_path):\n",
    "\ttest_scores = {}\n",
    "\ttest_scores_optimized = {}\n",
    "\t# Load Data\n",
    "\t# file_path = sys.argv[1]\n",
    "\tpos_train,neg_train,pos_test,neg_test,pos_vali,neg_vali,\\\n",
    "\tpos_train_ns,neg_train_ns,pos_test_ns,neg_test_ns,pos_vali_ns,neg_vali_ns = load_data(file_path) \n",
    "\n",
    "\t# Concat positive and negative data with labels\n",
    "\t# 1 for positive, 0 for negative\n",
    "\tpos_train['target'] = 1\n",
    "\tneg_train['target'] = 0\n",
    "\ttrain = pd.concat([pos_train,neg_train])\n",
    "\ttrain = shuffle(train, random_state=0).reset_index(drop=True)\n",
    "\n",
    "\tpos_test['target'] = 1\n",
    "\tneg_test['target'] = 0\n",
    "\ttest = pd.concat([pos_test,neg_test])\n",
    "\ttest = shuffle(test, random_state=0).reset_index(drop=True)\n",
    "\n",
    "\tpos_vali['target'] = 1\n",
    "\tneg_vali['target'] = 0\n",
    "\tvali = pd.concat([pos_vali,neg_vali])\n",
    "\tvali = shuffle(vali, random_state=0).reset_index(drop=True)\n",
    "\n",
    "\tpos_train_ns['target'] = 1\n",
    "\tneg_train_ns['target'] = 0\n",
    "\ttrain_ns = pd.concat([pos_train_ns,neg_train_ns])\n",
    "\ttrain_ns = shuffle(train_ns, random_state=0).reset_index(drop=True)\n",
    "\n",
    "\tpos_test_ns['target'] = 1\n",
    "\tneg_test_ns['target'] = 0\n",
    "\ttest_ns = pd.concat([pos_test_ns,neg_test_ns])\n",
    "\ttest_ns = shuffle(test_ns, random_state=0).reset_index(drop=True)\n",
    "\n",
    "\tpos_vali_ns['target'] = 1\n",
    "\tneg_vali_ns['target'] = 0\n",
    "\tvali_ns = pd.concat([pos_vali_ns,neg_vali_ns])\n",
    "\tvali_ns = shuffle(vali_ns, random_state=0).reset_index(drop=True)\n",
    "\n",
    "\t# Prepare x and y\n",
    "\tx_train = train['Review']\n",
    "\ty_train = train['target']\n",
    "\tx_test = test['Review']\n",
    "\ty_test = test['target']\n",
    "\tx_vali = vali['Review']\n",
    "\ty_vali = vali['target']\n",
    "\n",
    "\tx_train_ns = train_ns['Review']\n",
    "\ty_train_ns = train_ns['target']\n",
    "\tx_test_ns = test_ns['Review']\n",
    "\ty_test_ns = test_ns['target']\n",
    "\tx_vali_ns = vali_ns['Review']\n",
    "\ty_vali_ns = vali_ns['target']\n",
    "\n",
    "\t# 1. unigram\n",
    "\tclf, count_vect, tfidf = train_data(x_train, y_train, vect_unigram, 1)\n",
    "\ttest_scores['test_unigram'] = evaluate(x_test, y_test, clf, count_vect, tfidf)\n",
    "\n",
    "\tbest_alpha = alpha_tuning(x_train,y_train,x_vali,y_vali,vect_unigram,alpha_list)\n",
    "\tprint(\"The selected alpha:{}\\n\".format(best_alpha))\n",
    "\tclf, count_vect, tfidf = train_data(x_train, y_train, vect_unigram, best_alpha)\n",
    "\ttest_scores_optimized['test_unigram'] = evaluate(x_test, y_test, clf, count_vect, tfidf)\n",
    "\n",
    "\t# 2. bigram\n",
    "\tclf, count_vect, tfidf = train_data(x_train, y_train, vect_bigram, 1)\n",
    "\ttest_scores['test_bigram'] = evaluate(x_test, y_test, clf, count_vect, tfidf)\n",
    "\n",
    "\tbest_alpha = alpha_tuning(x_train,y_train,x_vali,y_vali,vect_bigram,alpha_list)\n",
    "\tprint(\"The selected alpha:{}\\n\".format(best_alpha))\n",
    "\tclf, count_vect, tfidf = train_data(x_train, y_train, vect_bigram, best_alpha)\n",
    "\ttest_scores_optimized['test_bigram'] = evaluate(x_test, y_test, clf, count_vect, tfidf)\n",
    "\n",
    "\t# 3. unigram + bigram\n",
    "\tclf, count_vect, tfidf = train_data(x_train, y_train, vect_unibigram, 1)\n",
    "\ttest_scores['test_unibigram'] = evaluate(x_test, y_test, clf, count_vect, tfidf)\n",
    "\n",
    "\tbest_alpha = alpha_tuning(x_train,y_train,x_vali,y_vali,vect_unibigram,alpha_list)\n",
    "\tprint(\"The selected alpha:{}\\n\".format(best_alpha))\n",
    "\tclf, count_vect, tfidf = train_data(x_train, y_train, vect_unibigram, best_alpha)\n",
    "\ttest_scores_optimized['test_unibigram'] = evaluate(x_test, y_test, clf, count_vect, tfidf)\n",
    "\n",
    "\t# 4. unigram(no stopword)\n",
    "\tclf, count_vect, tfidf = train_data(x_train_ns, y_train_ns, vect_unigram, 1)\n",
    "\ttest_scores['test_ns_uigram'] = evaluate(x_test_ns, y_test_ns, clf, count_vect, tfidf)\n",
    "\n",
    "\tbest_alpha = alpha_tuning(x_train,y_train,x_vali_ns,y_vali_ns,vect_unigram,alpha_list)\n",
    "\tprint(\"The selected alpha:{}\\n\".format(best_alpha))\n",
    "\tclf, count_vect, tfidf = train_data(x_train_ns, y_train_ns, vect_unigram, best_alpha)\n",
    "\ttest_scores_optimized['test_ns_unigram'] = evaluate(x_test_ns, y_test_ns, clf, count_vect, tfidf)\n",
    "\n",
    "\t# 5. bigram(no stopword)\n",
    "\tclf, count_vect, tfidf = train_data(x_train_ns, y_train_ns, vect_bigram, 1)\n",
    "\ttest_scores['test_ns_bigram'] = evaluate(x_test_ns, y_test_ns, clf, count_vect, tfidf)\n",
    "\n",
    "\tbest_alpha = alpha_tuning(x_train,y_train,x_vali_ns,y_vali_ns,vect_bigram,alpha_list)\n",
    "\tprint(\"The selected alpha:{}\\n\".format(best_alpha))\n",
    "\tclf, count_vect, tfidf = train_data(x_train_ns, y_train_ns, vect_bigram, best_alpha)\n",
    "\ttest_scores_optimized['test_ns_bigram'] = evaluate(x_test_ns, y_test_ns, clf, count_vect, tfidf)\n",
    "\n",
    "\t# 6. unigram + bigram(no stopword)\n",
    "\tclf, count_vect, tfidf = train_data(x_train_ns, y_train_ns, vect_unibigram, 1)\n",
    "\ttest_scores['test_ns_unibigram'] = evaluate(x_test_ns, y_test_ns, clf, count_vect, tfidf)\n",
    "\n",
    "\tbest_alpha = alpha_tuning(x_train,y_train,x_vali_ns,y_vali_ns,vect_unibigram,alpha_list)\n",
    "\tprint(\"The selected alpha:{}\\n\".format(best_alpha))\n",
    "\tclf, count_vect, tfidf = train_data(x_train_ns, y_train_ns, vect_unibigram, best_alpha)\n",
    "\ttest_scores_optimized['test_ns_unibigram'] = evaluate(x_test_ns, y_test_ns, clf, count_vect, tfidf)\n",
    "\n",
    "\treturn test_scores, test_scores_optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected alpha:1\n",
      "\n",
      "The selected alpha:0.1\n",
      "\n",
      "The selected alpha:0.1\n",
      "\n",
      "The selected alpha:0.001\n",
      "\n",
      "The selected alpha:0.1\n",
      "\n",
      "The selected alpha:0.01\n",
      "\n",
      "({'test_bigram': {'accuracy': 0.82195,\n",
      "                  'f1': 0.8279917884313488,\n",
      "                  'precision': 0.8008175659892548,\n",
      "                  'recall': 0.857075},\n",
      "  'test_ns_bigram': {'accuracy': 0.7869723371542144,\n",
      "                     'f1': 0.7971914792336071,\n",
      "                     'precision': 0.7607086077674313,\n",
      "                     'recall': 0.83735},\n",
      "  'test_ns_uigram': {'accuracy': 0.8061100763759547,\n",
      "                     'f1': 0.8108576097162438,\n",
      "                     'precision': 0.7914871330968648,\n",
      "                     'recall': 0.8312},\n",
      "  'test_ns_unibigram': {'accuracy': 0.8224727809097614,\n",
      "                        'f1': 0.8291180363373841,\n",
      "                        'precision': 0.7992113198793783,\n",
      "                        'recall': 0.86135},\n",
      "  'test_unibigram': {'accuracy': 0.83315,\n",
      "                     'f1': 0.8376787625255375,\n",
      "                     'precision': 0.815542716423565,\n",
      "                     'recall': 0.86105},\n",
      "  'test_unigram': {'accuracy': 0.809625,\n",
      "                   'f1': 0.8133120862956607,\n",
      "                   'precision': 0.7978595478595478,\n",
      "                   'recall': 0.829375}},\n",
      " {'test_bigram': {'accuracy': 0.8222875,\n",
      "                  'f1': 0.826488643714073,\n",
      "                  'precision': 0.8074015785583136,\n",
      "                  'recall': 0.8465},\n",
      "  'test_ns_bigram': {'accuracy': 0.7829847873098413,\n",
      "                     'f1': 0.7906269974312282,\n",
      "                     'precision': 0.7637410004892938,\n",
      "                     'recall': 0.819475},\n",
      "  'test_ns_unibigram': {'accuracy': 0.8043725546569332,\n",
      "                        'f1': 0.8090578560796466,\n",
      "                        'precision': 0.7901434631333111,\n",
      "                        'recall': 0.8289},\n",
      "  'test_ns_unigram': {'accuracy': 0.8013225165314567,\n",
      "                      'f1': 0.805134618213918,\n",
      "                      'precision': 0.7899865268020402,\n",
      "                      'recall': 0.820875},\n",
      "  'test_unibigram': {'accuracy': 0.83315,\n",
      "                     'f1': 0.8363172609996568,\n",
      "                     'precision': 0.8207374602868971,\n",
      "                     'recall': 0.8525},\n",
      "  'test_unigram': {'accuracy': 0.809625,\n",
      "                   'f1': 0.8133120862956607,\n",
      "                   'precision': 0.7978595478595478,\n",
      "                   'recall': 0.829375}})\n"
     ]
    }
   ],
   "source": [
    "%run main \"C:\\\\Users\\\\andre\\\\msci-text-analytics-s20\\\\Assignment 1\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
