{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSCI 641 Assignment 1 ----- Tianyu Shi ----- 20570373"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a python script to perform the following data preparation activities:\n",
    "### 1. Tokenize the corpu\n",
    "### 2. Remove the following special characters: !\"#$%&()*+/:;<=>@[\\\\]^`{|}~\\t\\n\n",
    "### 3. Create two versions of your dataset: (1) with stopwords and (2) without stopwords\n",
    "### 3. Randomly split your data into training (80%), validation (10%) and test (10%)\n",
    "\n",
    "### Write in core Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Using stop words from nltk\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the following special characters\n",
    "# Note: Use a quadruple backslash (\\\\\\\\) to remove the backslash\n",
    "special_characters = '[,.-?!\"#$%&(*)+/:;<=>@\\[\\]\\\\\\\\^`{|}~\\t\\n]+' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, characters, lower = 1, with_stop_words = 1):\n",
    "    # Remove the special characters\n",
    "    # set punctuations as separate tokens\n",
    "    for c in characters:\n",
    "        text = text.replace(c,\" \"+c+\" \")\n",
    "    # Split into tokens\n",
    "    if lower == 0:\n",
    "        tokens = text.split()\n",
    "    elif lower == 1:\n",
    "        # Normalize to lower case\n",
    "        tokens = text.lower().split()\n",
    "    else:\n",
    "        print(\"lower = 0: do not normalize to lower case \\n\")\n",
    "        print(\"lower = 1: normalize to lower case\")\n",
    "    if with_stop_words == 1:\n",
    "        return tokens\n",
    "    elif with_stop_words == 0:\n",
    "        tokens_without_sw = [w for w in tokens if not w in stop_words] \n",
    "        return tokens_without_sw\n",
    "    else:\n",
    "        print(\"with_stop_words = 0: without stopwords \\n\")\n",
    "        print(\"with_stop_words = 1: with stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training, validation, and test sets\n",
    "def split_data(data, training_size, test_size):\n",
    "    if training_size < 0 or test_size < 0:\n",
    "        print(\"Please input positive values\")\n",
    "    elif training_size + test_size > 1:\n",
    "        print(\"Please make sure that training_size + test_size <= 1\")\n",
    "    else:\n",
    "        # shuffle the data into random order\n",
    "        random.shuffle(data)\n",
    "        data_size = len(data)\n",
    "        training = data[:int(data_size*training_size)]\n",
    "        test = data[int(data_size*training_size):int(data_size*(training_size+test_size))]\n",
    "        validation = data[int(data_size*(training_size+test_size)):]\n",
    "        return training, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = open(\"C:\\\\Users\\\\andre\\MSCI 641\\\\pos.txt\")\n",
    "\n",
    "data = []\n",
    "data_no_stopword = []\n",
    "\n",
    "for line in pos:\n",
    "    tokens = tokenize(line, special_characters, lower = 1, with_stop_words = 1)\n",
    "    tokens_no_stopword = tokenize(line, special_characters, lower = 1, with_stop_words = 0)\n",
    "    data.append(tokens)\n",
    "    data_no_stopword.append(tokens_no_stopword)\n",
    "\n",
    "pos.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train, pos_test, pos_validation = split_data(data, 0.8, 0.1)\n",
    "pos_train_no_stopword, pos_test_no_stopword, pos_validation_no_stopword = split_data(data_no_stopword, 0.8, 0.1)\n",
    "\n",
    "save_path = \"C:\\\\Users\\\\andre\\\\MSCI 641\\\\Assignment 1\\\\\"\n",
    "np.savetxt(save_path + \"pos_train.csv\", pos_train, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(save_path + \"pos_test.csv\", pos_test, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(save_path + \"pos_validation.csv\", pos_validation, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(save_path + \"pos_train_no_stopword.csv\", pos_train_no_stopword, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(save_path + \"pos_test_no_stopword.csv\", pos_test_no_stopword, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(save_path + \"pos_validation_no_stopword.csv\", pos_validation_no_stopword, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = open(\"C:\\\\Users\\\\andre\\MSCI 641\\\\neg.txt\")\n",
    "\n",
    "data = []\n",
    "data_no_stopword = []\n",
    "\n",
    "for line in neg:\n",
    "    tokens = tokenize(line, special_characters, lower = 1, with_stop_words = 1)\n",
    "    tokens_no_stopword = tokenize(line, special_characters, lower = 1, with_stop_words = 0)\n",
    "    data.append(tokens)\n",
    "    data_no_stopword.append(tokens_no_stopword)\n",
    "\n",
    "neg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train, neg_test, neg_validation = split_data(data, 0.8, 0.1)\n",
    "neg_train_no_stopword, neg_test_no_stopword, neg_validation_no_stopword = split_data(data_no_stopword, 0.8, 0.1)\n",
    "\n",
    "save_path = \"C:\\\\Users\\\\andre\\\\MSCI 641\\\\Assignment 1\\\\\"\n",
    "np.savetxt(save_path + \"neg_train.csv\", neg_train, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(save_path + \"neg_test.csv\", neg_test, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(save_path + \"neg_validation.csv\", neg_validation, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(save_path + \"neg_train_no_stopword.csv\", neg_train_no_stopword, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(save_path + \"neg_test_no_stopword.csv\", neg_test_no_stopword, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(save_path + \"neg_validation_no_stopword.csv\", neg_validation_no_stopword, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
